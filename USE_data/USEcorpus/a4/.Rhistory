clean.targetFile <- targetFile %>% html_nodes("body") %>% html_text()
clean.targetFile = gsub("\r","",clean.targetFile)
clean.targetFile = gsub("\n"," ",clean.targetFile)
clean.targetFile = gsub("\t","",clean.targetFile)
clean.targetFile = gsub("'","",clean.targetFile)
clean.targetFile = gsub(",","",clean.targetFile)
clean.targetFile = gsub('\"', "",clean.targetFile) # could use some investigating to get a double quote rather than single quote...
# Assign data to the dataframe
USE_dataMerge =cbind(clean.targetFile, data.frame(i, clean.targetFile))
}
View(USE_dataMerge)
# Populate data with cleaned up texts
for (i in 1:iterations) {
# Get file from list, parse out text from HTML, remove weird markers
file = file(fileList[i])
targetFile = read_html(file)
clean.targetFile <- targetFile %>% html_nodes("body") %>% html_text()
clean.targetFile = gsub("\r","",clean.targetFile)
clean.targetFile = gsub("\n"," ",clean.targetFile)
clean.targetFile = gsub("\t","",clean.targetFile)
clean.targetFile = gsub("'","",clean.targetFile)
clean.targetFile = gsub(",","",clean.targetFile)
clean.targetFile = gsub('\"', "",clean.targetFile) # could use some investigating to get a double quote rather than single quote...
# Assign data to the dataframe
USE_dataMerge =rbind(i, clean.targetFile, data.frame(i, clean.targetFile))
}
View(USE_dataMerge)
USE_dataMerge = NULL
# Populate data with cleaned up texts
for (i in 1:iterations) {
# Get file from list, parse out text from HTML, remove weird markers
file = file(fileList[i])
targetFile = read_html(file)
clean.targetFile <- targetFile %>% html_nodes("body") %>% html_text()
clean.targetFile = gsub("\r","",clean.targetFile)
clean.targetFile = gsub("\n"," ",clean.targetFile)
clean.targetFile = gsub("\t","",clean.targetFile)
clean.targetFile = gsub("'","",clean.targetFile)
clean.targetFile = gsub(",","",clean.targetFile)
clean.targetFile = gsub('\"', "",clean.targetFile) # could use some investigating to get a double quote rather than single quote...
# Assign data to the dataframe
USE_dataMerge =rbind(i, clean.targetFile, data.frame(i, clean.targetFile))
}
View(USE_dataMerge)
USE_dataMerge = NULL
# Populate data with cleaned up texts
for (i in 1:iterations) {
# Get file from list, parse out text from HTML, remove weird markers
file = file(fileList[i])
targetFile = read_html(file)
clean.targetFile <- targetFile %>% html_nodes("body") %>% html_text()
clean.targetFile = gsub("\r","",clean.targetFile)
clean.targetFile = gsub("\n"," ",clean.targetFile)
clean.targetFile = gsub("\t","",clean.targetFile)
clean.targetFile = gsub("'","",clean.targetFile)
clean.targetFile = gsub(",","",clean.targetFile)
clean.targetFile = gsub('\"', "",clean.targetFile) # could use some investigating to get a double quote rather than single quote...
# Assign data to the dataframe
USE_dataMerge =rbind(data.frame(i, clean.targetFile))
}
View(USE_dataMerge)
USE_dataMerge = NULL
# Populate data with cleaned up texts
for (i in 1:iterations) {
# Get file from list, parse out text from HTML, remove weird markers
file = file(fileList[i])
targetFile = read_html(file)
clean.targetFile <- targetFile %>% html_nodes("body") %>% html_text()
clean.targetFile = gsub("\r","",clean.targetFile)
clean.targetFile = gsub("\n"," ",clean.targetFile)
clean.targetFile = gsub("\t","",clean.targetFile)
clean.targetFile = gsub("'","",clean.targetFile)
clean.targetFile = gsub(",","",clean.targetFile)
clean.targetFile = gsub('\"', "",clean.targetFile) # could use some investigating to get a double quote rather than single quote...
# Assign data to the dataframe
clean.targetFile =rbind(data.frame(i, clean.targetFile))
USE_dataMerge <- merge(USE_dataMerge,clean.targetFile)
}
View(USE_dataMerge)
View(targetFile)
USE_dataMerge = NULL
# Populate data with cleaned up texts
for (i in 1:iterations) {
# Get file from list, parse out text from HTML, remove weird markers
file = file(fileList[i])
targetFile = read_html(file)
clean.targetFile <- targetFile %>% html_nodes("body") %>% html_text()
clean.targetFile = gsub("\r","",clean.targetFile)
clean.targetFile = gsub("\n"," ",clean.targetFile)
clean.targetFile = gsub("\t","",clean.targetFile)
clean.targetFile = gsub("'","",clean.targetFile)
clean.targetFile = gsub(",","",clean.targetFile)
clean.targetFile = gsub('\"', "",clean.targetFile) # could use some investigating to get a double quote rather than single quote...
# Assign data to the dataframe
clean.targetFile =rbind(data.frame(i, clean.targetFile))
}
View(clean.targetFile)
USE_dataMerge = NULL
# Populate data with cleaned up texts
for (i in 1:iterations) {
# Get file from list, parse out text from HTML, remove weird markers
file = file(fileList[i])
targetFile = read_html(file)
clean.targetFile <- targetFile %>% html_nodes("body") %>% html_text()
clean.targetFile = gsub("\r","",clean.targetFile)
clean.targetFile = gsub("\n"," ",clean.targetFile)
clean.targetFile = gsub("\t","",clean.targetFile)
clean.targetFile = gsub("'","",clean.targetFile)
clean.targetFile = gsub(",","",clean.targetFile)
clean.targetFile = gsub('\"', "",clean.targetFile) # could use some investigating to get a double quote rather than single quote...
# Assign data to the dataframe
USE_dataMerge <- rbind(data.frame(i, clean.targetFile))
}
View(USE_dataMerge)
USE_dataMerge = NULL
# Populate data with cleaned up texts
for (i in 1:iterations) {
# Get file from list, parse out text from HTML, remove weird markers
file = file(fileList[i])
targetFile = read_html(file)
clean.targetFile <- targetFile %>% html_nodes("body") %>% html_text()
clean.targetFile = gsub("\r","",clean.targetFile)
clean.targetFile = gsub("\n"," ",clean.targetFile)
clean.targetFile = gsub("\t","",clean.targetFile)
clean.targetFile = gsub("'","",clean.targetFile)
clean.targetFile = gsub(",","",clean.targetFile)
clean.targetFile = gsub('\"', "",clean.targetFile) # could use some investigating to get a double quote rather than single quote...
# Assign data to the dataframe
USE_dataMerge[i] <- rbind(data.frame(i, clean.targetFile))
}
View(USE_dataMerge)
USE_dataMerge = NULL
# Populate data with cleaned up texts
for (i in 1:iterations) {
# Get file from list, parse out text from HTML, remove weird markers
file = file(fileList[i])
targetFile = read_html(file)
clean.targetFile <- targetFile %>% html_nodes("body") %>% html_text()
clean.targetFile = gsub("\r","",clean.targetFile)
clean.targetFile = gsub("\n"," ",clean.targetFile)
clean.targetFile = gsub("\t","",clean.targetFile)
clean.targetFile = gsub("'","",clean.targetFile)
clean.targetFile = gsub(",","",clean.targetFile)
clean.targetFile = gsub('\"', "",clean.targetFile) # could use some investigating to get a double quote rather than single quote...
# Assign data to the dataframe
temp = rbind(data.frame(i, clean.targetFile))
USE_dataMerge[i] <- c(temp)
}
View(USE_dataMerge)
USE_dataMerge = NULL
# Populate data with cleaned up texts
for (i in 1:iterations) {
# Get file from list, parse out text from HTML, remove weird markers
file = file(fileList[i])
targetFile = read_html(file)
clean.targetFile <- targetFile %>% html_nodes("body") %>% html_text()
clean.targetFile = gsub("\r","",clean.targetFile)
clean.targetFile = gsub("\n"," ",clean.targetFile)
clean.targetFile = gsub("\t","",clean.targetFile)
clean.targetFile = gsub("'","",clean.targetFile)
clean.targetFile = gsub(",","",clean.targetFile)
clean.targetFile = gsub('\"', "",clean.targetFile) # could use some investigating to get a double quote rather than single quote...
# Assign data to the dataframe
# temp = rbind(data.frame(i, clean.targetFile))
USE_dataMerge[i] <- c(i, clean.targetFile)
}
View(temp)
USE_dataMerge = NULL
# Populate data with cleaned up texts
for (i in 1:iterations) {
# Get file from list, parse out text from HTML, remove weird markers
file = file(fileList[i])
targetFile = read_html(file)
clean.targetFile <- targetFile %>% html_nodes("body") %>% html_text()
clean.targetFile = gsub("\r","",clean.targetFile)
clean.targetFile = gsub("\n"," ",clean.targetFile)
clean.targetFile = gsub("\t","",clean.targetFile)
clean.targetFile = gsub("'","",clean.targetFile)
clean.targetFile = gsub(",","",clean.targetFile)
clean.targetFile = gsub('\"', "",clean.targetFile) # could use some investigating to get a double quote rather than single quote...
# Assign data to the dataframe
# temp = rbind(data.frame(i, clean.targetFile))
USE_dataMerge[i,] <- i
USE_dataMerge[,i] <- clean.targetFile
}
USE_dataMerge = NULL
# Populate data with cleaned up texts
for (i in 1:iterations) {
# Get file from list, parse out text from HTML, remove weird markers
file = file(fileList[i])
targetFile = read_html(file)
clean.targetFile <- targetFile %>% html_nodes("body") %>% html_text()
clean.targetFile = gsub("\r","",clean.targetFile)
clean.targetFile = gsub("\n"," ",clean.targetFile)
clean.targetFile = gsub("\t","",clean.targetFile)
clean.targetFile = gsub("'","",clean.targetFile)
clean.targetFile = gsub(",","",clean.targetFile)
clean.targetFile = gsub('\"', "",clean.targetFile) # could use some investigating to get a double quote rather than single quote...
# Assign data to the dataframe
# temp = rbind(data.frame(i, clean.targetFile))
USE_dataMerge[i] <- clean.targetFile
}
USE_dataMerge[1]
USE_dataMerge[2]
length(USE_dataMerge)
# Populate data with cleaned up texts
for (i in 1:iterations) {
# Get file from list, parse out text from HTML, remove weird markers
file = file(fileList[i])
targetFile = read_html(file)
clean.targetFile <- targetFile %>% html_nodes("body") %>% html_text()
clean.targetFile = gsub("\r","",clean.targetFile)
clean.targetFile = gsub("\n"," ",clean.targetFile)
clean.targetFile = gsub("\t","",clean.targetFile)
clean.targetFile = gsub("'","",clean.targetFile)
clean.targetFile = gsub(",","",clean.targetFile)
clean.targetFile = gsub('\"', "",clean.targetFile) # could use some investigating to get a double quote rather than single quote...
# Assign data to the dataframe
# temp = rbind(data.frame(i, clean.targetFile))
USE_dataMerge[i] <- clean.targetFile
USE_dataMerge[id] <- i
}
# Populate data with cleaned up texts
for (i in 1:iterations) {
# Get file from list, parse out text from HTML, remove weird markers
file = file(fileList[i])
targetFile = read_html(file)
clean.targetFile <- targetFile %>% html_nodes("body") %>% html_text()
clean.targetFile = gsub("\r","",clean.targetFile)
clean.targetFile = gsub("\n"," ",clean.targetFile)
clean.targetFile = gsub("\t","",clean.targetFile)
clean.targetFile = gsub("'","",clean.targetFile)
clean.targetFile = gsub(",","",clean.targetFile)
clean.targetFile = gsub('\"', "",clean.targetFile) # could use some investigating to get a double quote rather than single quote...
# Assign data to the dataframe
# temp = rbind(data.frame(i, clean.targetFile))
USE_dataMerge[i] <- clean.targetFile
USE_dataMerge$id <- i
}
View(USE_dataMerge)
USE_dataMerge = NULL
# Populate data with cleaned up texts
for (i in 1:iterations) {
# Get file from list, parse out text from HTML, remove weird markers
file = file(fileList[i])
targetFile = read_html(file)
clean.targetFile <- targetFile %>% html_nodes("body") %>% html_text()
clean.targetFile = gsub("\r","",clean.targetFile)
clean.targetFile = gsub("\n"," ",clean.targetFile)
clean.targetFile = gsub("\t","",clean.targetFile)
clean.targetFile = gsub("'","",clean.targetFile)
clean.targetFile = gsub(",","",clean.targetFile)
clean.targetFile = gsub('\"', "",clean.targetFile) # could use some investigating to get a double quote rather than single quote...
# Assign data to the dataframe
# temp = rbind(data.frame(i, clean.targetFile))
USE_dataMerge$text <- clean.targetFile
USE_dataMerge$id <- i
}
View(USE_dataMerge)
USE_dataMerge[1]
USE_dataMerge[2]
# Populate data with cleaned up texts
for (i in 1:iterations) {
# Get file from list, parse out text from HTML, remove weird markers
file = file(fileList[i])
targetFile = read_html(file)
clean.targetFile <- targetFile %>% html_nodes("body") %>% html_text()
clean.targetFile = gsub("\r","",clean.targetFile)
clean.targetFile = gsub("\n"," ",clean.targetFile)
clean.targetFile = gsub("\t","",clean.targetFile)
clean.targetFile = gsub("'","",clean.targetFile)
clean.targetFile = gsub(",","",clean.targetFile)
clean.targetFile = gsub('\"', "",clean.targetFile) # could use some investigating to get a double quote rather than single quote...
# Assign data to the dataframe
# temp = rbind(data.frame(i, clean.targetFile))
USE_dataMerge[i] <- clean.targetFile
}
View(USE_dataMerge)
USE_dataMerge = NULL
# Populate data with cleaned up texts
for (i in 1:iterations) {
# Get file from list, parse out text from HTML, remove weird markers
file = file(fileList[i])
targetFile = read_html(file)
clean.targetFile <- targetFile %>% html_nodes("body") %>% html_text()
clean.targetFile = gsub("\r","",clean.targetFile)
clean.targetFile = gsub("\n"," ",clean.targetFile)
clean.targetFile = gsub("\t","",clean.targetFile)
clean.targetFile = gsub("'","",clean.targetFile)
clean.targetFile = gsub(",","",clean.targetFile)
clean.targetFile = gsub('\"', "",clean.targetFile) # could use some investigating to get a double quote rather than single quote...
# Assign data to the dataframe
# temp = rbind(data.frame(i, clean.targetFile))
USE_dataMerge[i] <- clean.targetFile
}
USE_dataMerge
USE_positive = NULL
USE_negative = NULL
i = 1
# Remove stop words
USE_nostop <- tokens(USE_dataMerge[i])
USE_nostop
USE_nostop <- tokens_select(USE_nostop, pattern = stopwords("en"), selection = "remove")
USE_nostop
positive = sents[which(sents[,2]=='positive'),1]$word
# Sentiment analysis
#testing
sents = get_sentiments("nrc")
positive = sents[which(sents[,2]=='positive'),1]$word
negative = sents[which(sents[,2]=='negative'),1]$word
USE_nostop$joy = 0
USE_nostop <- gsub(",","", toString(USE_nostop))
USE_nostop = tolower(unlist(strsplit(USE_nostop," ")))
m_positive = mean(USE_nostop %in% positive)
m_positive
m_negative = mean(USE_nostop %in% negative)
m_negative
USE_nostop$positive = 0
USE_nostop$negative = 0
USE_nostop <- gsub(",","", toString(USE_nostop))
USE_nostop = tolower(unlist(strsplit(USE_nostop," ")))
m_positive = mean(USE_nostop %in% positive)
USE_nostop$positive = m_positive
USE_nostop$positive
m_negative = mean(USE_nostop %in% negative)
USE_nostop$negative = m_negative
USE_nostop$negative
View(USE_nostop)
USE1_positive = 0
USE1_negative = 0
for (i in 1:iterations) {
USE_nostop <- gsub(",","", toString(USE_nostop))
USE_nostop = tolower(unlist(strsplit(USE_nostop," ")))
m_positive = mean(USE_nostop %in% positive)
USE1_positive = rbind(m_positive, USE1_positive)
USE1_positive
m_negative = mean(USE_nostop %in% negative)
USE1_positive = rbind(m_negative, USE1_negative)
USE1_negative
}
USE1_negative
USE1_positive
for (i in 1:iterations) {
USE_nostop <- gsub(",","", toString(USE_nostop))
USE_nostop = tolower(unlist(strsplit(USE_nostop," ")))
m_positive = mean(USE_nostop %in% positive)
USE1_positive = rbind(m_positive, USE1_positive)
USE1_positive
m_negative = mean(USE_nostop %in% negative)
USE1_negative = rbind(m_negative, USE1_negative)
USE1_negative
}
USE1_positive
length(USE1_positive)
for (i in 1:iterations) {
USE_nostop <- gsub(",","", toString(USE_dataMerge[i]))
USE_nostop = tolower(unlist(strsplit(USE_nostop," ")))
m_positive = mean(USE_nostop %in% positive)
USE1_positive = rbind(m_positive, USE1_positive)
USE1_positive
m_negative = mean(USE_nostop %in% negative)
USE1_negative = rbind(m_negative, USE1_negative)
USE1_negative
}
length(USE1_negative)
USE1_positive = NULL
USE1_negative = NULL
for (i in 1:iterations) {
USE_nostop <- gsub(",","", toString(USE_dataMerge[i]))
USE_nostop = tolower(unlist(strsplit(USE_nostop," ")))
m_positive = mean(USE_nostop %in% positive)
USE1_positive = rbind(m_positive, USE1_positive)
USE1_positive
m_negative = mean(USE_nostop %in% negative)
USE1_negative = rbind(m_negative, USE1_negative)
USE1_negative
}
USE1_positive
length(USE1_positive)
?t.test
t.test(USE1_positive, USE1_negative)
USE_dataMerge[i]
for (i in 1:iterations) {
# USE_nostop <- gsub(",","", toString(USE_dataMerge[i]))
# USE_nostop = tolower(unlist(strsplit(USE_nostop," ")))
USE_nostop <- tokens(USE_dataMerge[i])
USE_nostop <- tokens_select(USE_nostop, pattern = stopwords("en"), selection = "remove")
m_positive = mean(USE_nostop %in% positive)
USE1_positive = rbind(m_positive, USE1_positive)
USE1_positive
m_negative = mean(USE_nostop %in% negative)
USE1_negative = rbind(m_negative, USE1_negative)
USE1_negative
}
USE1_positive
USE1_positive = NULL
USE1_negative = NULL
for (i in 1:iterations) {
# USE_nostop <- gsub(",","", toString(USE_dataMerge[i]))
# USE_nostop = tolower(unlist(strsplit(USE_nostop," ")))
USE_nostop <- tokens(USE_dataMerge[i])
USE_nostop <- tokens_select(USE_nostop, pattern = stopwords("en"), selection = "remove")
m_positive = mean(USE_nostop %in% positive)
USE1_positive = rbind(m_positive, USE1_positive)
USE1_positive
m_negative = mean(USE_nostop %in% negative)
USE1_negative = rbind(m_negative, USE1_negative)
USE1_negative
}
USE1_positive
View(USE_nostop)
USE_nostop <- gsub(",","", toString(USE_dataMerge[i]))
USE_nostop
USE_nostop = tolower(unlist(strsplit(USE_nostop," ")))
USE_nostop
USE_nostop2 <- tokens(USE_dataMerge[i])
USE_nostop2
USE_nostop <- tokens_select(USE_nostop, pattern = stopwords("en"), selection = "remove")
USE_nostop = NULL
for (i in 1:iterations) {
# USE_nostop <- gsub(",","", toString(USE_dataMerge[i]))
# USE_nostop = tolower(unlist(strsplit(USE_nostop," ")))
USE_nostop <- tokens(USE_dataMerge[i])
USE_nostop <- tokens_select(USE_nostop, pattern = stopwords("en"), selection = "remove")
m_positive = mean(USE_nostop %in% positive)
USE1_positive = rbind(m_positive, USE1_positive)
USE1_positive
m_negative = mean(USE_nostop %in% negative)
USE1_negative = rbind(m_negative, USE1_negative)
USE1_negative
}
USE1_positive
i
USE_nostop <- tokens(USE_dataMerge[i])
USE_nostop
USE_nostop <- tokens_select(USE_nostop, pattern = stopwords("en"), selection = "remove")
USE_nostop
m_positive = mean(USE_nostop %in% positive)
m_positive
USE_nostop <- tokens(USE_dataMerge[i])
USE_nostop <- tokens_select(USE_nostop, pattern = stopwords("en"), selection = "remove")
USE_nostop = tolower(unlist(strsplit(USE_nostop," ")))
USE_nostop = tolower(unlist(USE_nostop," ")))
USE_nostop = tolower(unlist(USE_nostop," "))
USE_nostop
m_positive = mean(USE_nostop %in% positive)
USE1_positive = rbind(m_positive, USE1_positive)
USE1_positive
USE_nostop <- gsub(",","", toString(USE_dataMerge[i]))
USE_nostop = tolower(unlist(strsplit(USE_nostop," ")))
m_positive = mean(USE_nostop %in% positive)
USE1_positive = rbind(m_positive, USE1_positive)
USE1_positive
USE1_positive = NULL
USE1_negative = NULL
USE_nostop <- gsub(",","", toString(USE_dataMerge[i]))
USE_nostop = tolower(unlist(strsplit(USE_nostop," ")))
m_positive = mean(USE_nostop %in% positive)
USE1_positive = rbind(m_positive, USE1_positive)
USE1_positive
typeof(USE_nostop)
USE_nostop = NULL
USE_nostop <- gsub(",","", toString(USE_dataMerge[i]))
USE_nostop
USE_nostop = tolower(unlist(strsplit(USE_nostop," ")))
USE_nostop <- USE_nostop %>%
anti_join(get_stopwords())
?anti_join
install.packages("dplyr")
install.packages("dplyr")
library(dplyr) # Maybe?
USE_nostop <- USE_nostop %>%
anti_join(get_stopwords())
?stopwords
?qdap
install.packages("qdap")
library(qdap) # Maybe?
USE_nostop = NULL
USE_nostop <- gsub(",","", toString(USE_dataMerge[i]))
USE_nostop = tolower(unlist(strsplit(USE_nostop," ")))
USE_nostop <- stopwords(USE_nostop, tm::stopwords("e"))
?stopwords
packages.install("stopwords")
library(qdap) # Maybe?
install.packages("qdap")
library(qdap) # Maybe?
?stop_words
USE_nostop <- stopwords(USE_nostop, tm::stopwords("e"))
USE_nostop
install.packages("stopwords")
library(stopwords) # Maybe?
head(stopwords::stopwords("en", source = "snowball"))
stopwords("en")
# Remove stop words
USE_nostop <- tokens(USE_dataMerge[i])
?tokens
USE_nostop <- gsub(",","", toString(USE_dataMerge[i]))
USE_nostop = tolower(unlist(strsplit(USE_nostop," ")))
m_positive = mean(USE_nostop %in% positive)
USE1_positive = rbind(m_positive, USE1_positive)
USE1_positive
USE1_positive = NULL
USE1_negative = NULL
USE_nostop = NULL
USE_nostop <- gsub(",","", toString(USE_dataMerge[i]))
USE_nostop = tolower(unlist(strsplit(USE_nostop," ")))
m_positive = mean(USE_nostop %in% positive)
USE1_positive = rbind(m_positive, USE1_positive)
USE1_positive
m_negative = mean(USE_nostop %in% negative)
USE1_negative = rbind(m_negative, USE1_negative)
USE1_negative
